{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine-tuning example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TED3lkhAYQIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from tfrecord.torch.dataset import TFRecordDataset\n",
        "\n",
        "tfrecord_path = \"train_samples.tfrecord\"\n",
        "index_path = None\n",
        "#description = {\"image\": \"byte\", \"label\": \"float\"}\n",
        "dataset = TFRecordDataset(tfrecord_path, index_path)#, description)\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
        "\n",
        "#data = next(iter(loader))\n",
        "#print(data.keys)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agd7VIxJ2zAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "\n",
        "from absl import logging\n",
        "\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TYDIQA(BertPreTrainedModel):\n",
        "    \"Create a QA model for tydi taks\"\n",
        "\n",
        "    def __init__(self, bert_config):\n",
        "        super(BertPreTrainedModel, self).__init__(bert_config)\n",
        "        self.num_answer_types = 5\n",
        "\n",
        "        self.bert = BertModel(bert_config)\n",
        "\n",
        "        self.qa_outputs = nn.Linear(bert_config.hidden_size, 2) #we need to label start and end position\n",
        "        self.answer_type_output_dense = nn.Linear(bert_config.hidden_size, self.num_answer_types)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids = None,\n",
        "                attention_mask = None,\n",
        "                token_type_ids = None,\n",
        "                position_ids = None,\n",
        "                head_mask = None,\n",
        "                inputs_embeds = None,\n",
        "                start_positions = None,\n",
        "                end_positions = None,\n",
        "                answer_types = None\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim = -1) #split logits into two, with each of size [batch * seq_len]\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        # Get the logits for the answer type prediction.\n",
        "        answer_type_output_layer = outputs[1]\n",
        "        answer_type_logits = self.answer_type_output_dense(answer_type_output_layer)\n",
        "\n",
        "        #get sequence length\n",
        "        seq_length = sequence_output.size(1)\n",
        "\n",
        "        def compute_loss(logits, positions):\n",
        "            one_hot_positions = F.one_hot(\n",
        "                positions, num_classes = seq_length\n",
        "            )\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            loss = -torch.mean(torch.sum(one_hot_positions * log_probs, dim = -1))\n",
        "            return loss\n",
        "\n",
        "        # Computes the loss for labels.\n",
        "        def compute_label_loss(logits, labels):\n",
        "            one_hot_positions = F.one_hot(\n",
        "                labels, num_classes=self.num_answer_types\n",
        "            )\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            loss = -torch.mean(torch.sum(one_hot_positions * log_probs, dim=-1))\n",
        "            return loss\n",
        "\n",
        "        start_loss = compute_loss(start_logits, start_positions)\n",
        "        end_loss = compute_loss(end_logits, end_positions)\n",
        "\n",
        "        answer_type_loss = compute_label_loss(answer_type_logits, answer_types)\n",
        "\n",
        "        total_loss = (start_loss + end_loss + answer_type_loss) / 3.0\n",
        "\n",
        "        return start_logits, end_logits, answer_type_logits, total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMjXTL5u30Mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfrecord_path = \"train_samples.tfrecord\"\n",
        "#index_path = None\n",
        "#description = {\"image\": \"byte\", \"label\": \"float\"}\n",
        "dataset = TFRecordDataset(tfrecord_path, index_path)#, description)\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
        "model = TYDIQA.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yba74phavtMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0fcecc28-582d-439a-9164-255f591eb8fb"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f436f3338d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJVo1y4YQ9vC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "f1f4468b-5994-401b-f126-8612519a2d52"
      },
      "source": [
        "outputs[-1]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ca882dd30a6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LiWbqp96LOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import json\n",
        "import os\n",
        "\n",
        "from absl import logging\n",
        "from bert import modeling as bert_modeling\n",
        "import tensorflow.compat.v1 as tf\n",
        "import postproc\n",
        "\n",
        "import torch\n",
        "import argparse\n",
        "from transformers import BertConfig, AdamW, get_linear_schedule_with_warmup, squad_convert_examples_to_features\n",
        "#from tydi_modeling_torch import TYDIQA\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import tqdm\n",
        "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Arguments for running tydi')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--predict_file\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=False,\n",
        "         help=\"TyDi json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz. \"\n",
        "         \"Used only for `--do_predict`.\"\n",
        "    )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--precomputed_predict_file\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=False,\n",
        "         help=\"TyDi json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz. \"\n",
        "         \"Used only for `--do_predict`.\"\n",
        "    )\n",
        "\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--max_seq_length\",\n",
        "        default=512,\n",
        "        type=int,\n",
        "        required=False,\n",
        "         help=\"Where to print predictions in TyDi prediction format, to be passed to\"\n",
        "         \"tydi_eval.py.\"\n",
        "    )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--doc_stride\",\n",
        "        default=128,\n",
        "        type=int,\n",
        "        required=False,\n",
        "         help=\"When splitting up a long document into chunks, how much stride to \"\n",
        "         \"take between chunks.\"\n",
        "    )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--max_question_length\",\n",
        "        default=64,\n",
        "        type=str,\n",
        "        required=False,\n",
        "         help=\"When splitting up a long document into chunks, how much stride to \"\n",
        "         \"take between chunks.\"\n",
        "    )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--do_train\",\n",
        "        default=True,\n",
        "        type=bool,\n",
        "        required=False,\n",
        "         help=\"Whether to run training.\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--train_batch_size\",\n",
        "        default=1,\n",
        "        type=int,\n",
        "        required=False,\n",
        "         help=\"Whether to run prediction.\"\n",
        "    )\n",
        "\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--predict_batch_size\",\n",
        "        default=16,\n",
        "        type=int,\n",
        "        required=False,\n",
        "         help=\"Total batch size for predictions.\"\n",
        "    )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--predict_file_shard_size\",\n",
        "        default=1000,\n",
        "        type=int,\n",
        "        required=False,\n",
        "         help=\"The maximum number of examples to put into each temporary TF example file \"\n",
        "    \"used as model input a prediction time.\"\n",
        "    )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--learning_rate\",\n",
        "        default=5e-5,\n",
        "        type=float,\n",
        "        required=False,\n",
        "         help=\"The initial learning rate for Adam.\"\n",
        "    )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--num_train_epochs\",\n",
        "        default=3,\n",
        "        type=int,\n",
        "        required=False,\n",
        "         help=\"The initial learning rate for Adam.\"\n",
        "    )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--warmup_proportion\",\n",
        "        default=0.1,\n",
        "        type=float,\n",
        "        required=False,\n",
        "         help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
        "    \"E.g., 0.1 = 10% of training.\"\n",
        "    )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--save_checkpoints_steps\",\n",
        "        default=0.1,\n",
        "        type=float,\n",
        "        required=False,\n",
        "         help=\"How often to save the model checkpoint.\"\n",
        "    \"E.g., 0.1 = 10% of training.\"\n",
        "    )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--iterations_per_loop\",\n",
        "        default=1000,\n",
        "        type=int,\n",
        "        required=False,\n",
        "         help=\"How many steps to make in each estimator call.\"\n",
        "    )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--max_answer_length\",\n",
        "        default=30,\n",
        "        type=int,\n",
        "        required=False,\n",
        "         help=\"The maximum length of an answer that can be generated. This is needed \"\n",
        "    \"because the start and end predictions are not conditioned on one another.\"\n",
        "    )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--include_unknowns\",\n",
        "        default=-1.0,\n",
        "        type=float,\n",
        "        required=False,\n",
        "         help=   \"If positive, probability of including answers of type `UNKNOWN`.\"\n",
        "         )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--verbose_logging\",\n",
        "        default=False,\n",
        "        type=bool,\n",
        "        required=False,\n",
        "         help=   \"If true, all of the warnings related to data processing will be printed. \"\n",
        "    \"A number of warnings are expected for a normal TyDi evaluation.\"\n",
        "         )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--max_passages\",\n",
        "        default=45,\n",
        "        type=int,\n",
        "        required=False,\n",
        "         help=   \"Maximum number of passages to consider for a \"\n",
        "                        \"single article. If an article contains more than\"\n",
        "                        \"this, they will be discarded during training. \"\n",
        "                        \"BERT's WordPiece vocabulary must be modified to include \"\n",
        "                        \"these within the [unused*] vocab IDs.\"\n",
        "         )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--max_position\",\n",
        "        default=45,\n",
        "        type=int,\n",
        "        required=False,\n",
        "         help=  \"Maximum passage position for which to generate special tokens.\"\n",
        "         )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--fail_on_invalid\",\n",
        "        default=True,\n",
        "        type=bool,\n",
        "        required=False,\n",
        "         help= \"Stop immediately on encountering an invalid example? \"\n",
        "    \"If false, just print a warning and skip it.\"\n",
        "         )\n",
        "\n",
        "parser.add_argument(\n",
        "        \"--adam_epsilon\",\n",
        "        default=1e-8,\n",
        "        type=float,\n",
        "        required=False,\n",
        "         help= \"weight decaying rate for adam optimizer\"\n",
        "         )\n",
        "\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--gradient_accumulation_steps\",\n",
        "    type=int,\n",
        "    default=1,\n",
        "    help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        ")\n",
        "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n",
        "parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7H54_w-YoMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "25242620-e160-4cc2-f036-914edc783df5"
      },
      "source": [
        "train(args, dataset, model)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:***** Running training *****\n",
            "INFO:absl:  Num Epochs = 3\n",
            "INFO:absl:  Let's start finetuning!\n",
            "Epoch: 100%|██████████| 3/3 [05:44<00:00, 114.77s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45, 1.9299078902436628)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Tf-0tqBtoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, train_dataset, model):\n",
        "    # use tensorboard to keep track of training process\n",
        "    tb_writer = SummaryWriter()\n",
        "\n",
        "    #train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size)\n",
        "\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "\n",
        "    # Train!\n",
        "    logging.info(\"***** Running training *****\")\n",
        "    #logging.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logging.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "\n",
        "    logging.info(\"  Let's start finetuning!\")\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    global_step = 0\n",
        "    for epoch in tqdm.trange(args.num_train_epochs, desc = 'Epoch'):\n",
        "        #epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            model.train()\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"].long().to(DEVICE),\n",
        "                attention_mask=batch['input_mask'].long().to(DEVICE),\n",
        "                token_type_ids=batch['segment_ids'].long().to(DEVICE),\n",
        "                start_positions=batch[\"start_positions\"].long().to(DEVICE),\n",
        "                end_positions=batch[\"end_positions\"].long().to(DEVICE),\n",
        "                answer_types=batch[\"answer_types\"].long().to(DEVICE)\n",
        "            )\n",
        "\n",
        "            loss = outputs[-1]\n",
        "\n",
        "            loss.backward()\n",
        "            tr_loss += loss.item()\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "            global_step += 1\n",
        "\n",
        "            #loggin points\n",
        "            if global_step % args.logging_steps == 0:\n",
        "                tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "                logging_loss = tr_loss\n",
        "\n",
        "            # save checkpoint\n",
        "            if global_step % args.save_steps == 0:\n",
        "                output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "                if not os.path.exists(output_dir):\n",
        "                    os.makedirs(output_dir)\n",
        "                    # Take care of distributed/parallel training\n",
        "                model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "                model_to_save.save_pretrained(output_dir)\n",
        "                tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                logging.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                logging.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmmXSOFs6QKn",
        "colab_type": "code",
        "outputId": "d734d987-9281-45b4-fea3-4458f3d0c3ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "dict_keys(['answer_types', 'segment_ids', 'input_ids', 'input_mask', 'language_id', 'start_positions', 'end_positions', 'unique_ids', 'example_index'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbert\u001b[0m/                            run_tydi.py\n",
            "data.py                          run_tydi_test.py\n",
            "debug.py                         tf_io.py\n",
            "\u001b[01;34mdev_samples\u001b[0m/                     tiny_dev.jsonl.gz\n",
            "mbert_modified_vocab.txt         tokenization.py\n",
            "\u001b[01;34mmulti_cased_L-12_H-768_A-12\u001b[0m/     train_samples_record_count.txt\n",
            "multi_cased_L-12_H-768_A-12.zip  train_samples.tfrecord\n",
            "postproc.py                      tydi_modeling.py\n",
            "prepare_tydi_data.py             \u001b[01;32mtydiqa.tape\u001b[0m*\n",
            "preproc.py                       tydiqa-v1.0-dev.jsonl.gz\n",
            "\u001b[01;34m__pycache__\u001b[0m/                     tydiqa-v1.0-train.jsonl.gz\n",
            "README.md\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}