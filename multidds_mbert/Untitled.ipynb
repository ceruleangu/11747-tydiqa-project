{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanzy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-0c1a574436ea>:26: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "dataset = input_fn_builder('/Users/chanzy/Desktop/tydiqa/tydiqa/baseline/train_samples.tfrecord', 512, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
    "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "    # This needs to be kept in sync with `FeatureWriter`.\n",
    "    name_to_features = {\n",
    "      \"language_id\": tf.FixedLenFeature([], tf.int64),\n",
    "      \"unique_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "    }\n",
    "\n",
    "    if is_training:\n",
    "        name_to_features[\"start_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
    "        name_to_features[\"end_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
    "        name_to_features[\"answer_types\"] = tf.FixedLenFeature([], tf.int64)\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "        # So cast all int64 to int32.\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.to_int32(t)\n",
    "                example[name] = t\n",
    "\n",
    "        return example\n",
    "\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    d = tf.data.TFRecordDataset(input_file)\n",
    "    d = d.shuffle(buffer_size=100)\n",
    "    d = d.map(lambda record: _decode_record(record, name_to_features))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataset_fn(ds, i): \n",
    "    return ds.filter(lambda x: x['language_id']==tf.to_int32(i))\n",
    "data_set_lst = []\n",
    "for i in range(11):\n",
    "    dataset_filter_lang = dataset.apply(lambda x: dataset_fn(x, i))\n",
    "    data_set_lst.append(dataset_filter_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_dev(dataset, train_port, num_dev):\n",
    "    train_size = int(DATASET_SIZE * train_port)\n",
    "    sub_dev_size = int(DATASET_SIZE * (1-train_port)/num_dev)\n",
    "    \n",
    "    full_dataset = dataset.shuffle(100)\n",
    "    train_dataset = full_dataset.take(train_size)\n",
    "    test_dataset = full_dataset.skip(train_size)\n",
    "    \n",
    "    shard_devs = []\n",
    "    for i in range(num_dev):\n",
    "        shard_devs.append(test_dataset.shard(num_shards=num_dev, index=i))\n",
    "    return train_dataset, shard_devs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = 41\n",
    "train_dataset, dev_shards = split_train_dev(dataset, 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=7>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))['language_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_dataset(dataset):\n",
    "    cnt = 0\n",
    "    for i in dataset:\n",
    "        #if cnt % 2000==0:\n",
    "            #print(cnt)\n",
    "        cnt += 1\n",
    "    \n",
    "    return cnt\n",
    "count_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dataset(dev_shards[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_langs(dataset):\n",
    "    def dataset_fn(ds, i):\n",
    "        return ds.filter(lambda x: x['language_id'] == i)\n",
    "    data_set_lst = []\n",
    "    for i in range(12):\n",
    "        dataset_filter_lang = dataset.apply(lambda x: dataset_fn(x, i))\n",
    "        data_set_lst.append(dataset_filter_lang)\n",
    "    return data_set_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_langs = split_langs(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dataset(train_set_langs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in train_set_langs:\n",
    "    print(count_dataset(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set_langs = []\n",
    "for div_set in dev_shards:\n",
    "    dev_set_langs.append(split_langs(div_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_sampler = list(map(lambda x: iter(x.repeat()), train_set_langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_set_sampler = []\n",
    "for dev_set in dev_set_langs:\n",
    "    dev_set_sampler.append(list(map(lambda x: iter(x.repeat()), dev_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=-1085280860>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
    "\n",
    "  # Implements linear decay of the learning rate.\n",
    "  learning_rate = tf.train.polynomial_decay(\n",
    "      learning_rate,\n",
    "      global_step,\n",
    "      num_train_steps,\n",
    "      end_learning_rate=0.0,\n",
    "      power=1.0,\n",
    "      cycle=False)\n",
    "\n",
    "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
    "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "  if num_warmup_steps:\n",
    "    global_steps_int = tf.cast(global_step, tf.int32)\n",
    "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
    "\n",
    "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
    "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
    "\n",
    "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "    warmup_learning_rate = init_lr * warmup_percent_done\n",
    "\n",
    "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
    "    learning_rate = (\n",
    "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
    "\n",
    "  # It is recommended that you use this optimizer for fine tuning, since this\n",
    "  # is how the model was trained (note that the Adam m/v variables are NOT\n",
    "  # loaded from init_checkpoint.)\n",
    "  optimizer = AdamWeightDecayOptimizer(\n",
    "      learning_rate=learning_rate,\n",
    "      weight_decay_rate=0.01,\n",
    "      beta_1=0.9,\n",
    "      beta_2=0.999,\n",
    "      epsilon=1e-6,\n",
    "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
    "\n",
    "  if use_tpu:\n",
    "    optimizer = contrib_tpu.CrossShardOptimizer(optimizer)\n",
    "\n",
    "  tvars = tf.trainable_variables()\n",
    "  grads = tf.gradients(loss, tvars)\n",
    "\n",
    "  # This is how the model was pre-trained.\n",
    "  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "\n",
    "  train_op = optimizer.apply_gradients(\n",
    "      zip(grads, tvars), global_step=global_step)\n",
    "\n",
    "  # Normally the global step update is done inside of `apply_gradients`.\n",
    "  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
    "  # a different optimizer, you should probably take this line out.\n",
    "  new_global_step = global_step + 1\n",
    "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = tf.get_variable(\n",
    "\"phi\", [10], initializer=tf.truncated_normal_initializer(stddev=0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.log(tf.nn.softmax(phi))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.log(tf.nn.softmax(phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1017699,\n",
       " 0.099157505,\n",
       " 0.100775644,\n",
       " 0.09762852,\n",
       " 0.09632411,\n",
       " 0.10006377,\n",
       " 0.10062185,\n",
       " 0.10204338,\n",
       " 0.09812517,\n",
       " 0.10349009]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tf.nn.softmax(phi).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    loss = tf.log(tf.nn.softmax(phi))\n",
    "grads = tape.gradient(loss, phi)\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = tf.log(tf.nn.softmax(phi))\n",
    "grads2 = tape.gradient(loss, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_a = tf.nn.l2_normalize(grads,0)        \n",
    "normalize_b = tf.nn.l2_normalize(grads2,0)\n",
    "cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([-0.00880536, -0.04625217,  0.01688722, -0.00595451,  0.0193012 ,\n",
       "       -0.00927766,  0.01762019,  0.01860519,  0.02450252, -0.02662662],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity * grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    loss = tf.log(tf.nn.softmax(phi))\n",
    "grads2 = tape.gradient(tf.nn.softmax(phi)[0], phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([0.99984336, 0.99996454, 0.9999699 , 0.99971884, 0.99932444,\n",
       "       0.9999998 , 0.9999807 , 0.99979126, 0.9998242 , 0.999391  ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.cos(grads, grads2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(grads2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_step = tf.train.get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fase\n"
     ]
    }
   ],
   "source": [
    "if global_step == 0:\n",
    "    print('fase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(\n",
    "    learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False,\n",
    "    name='Adam'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
